{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from keras import optimizers\n",
    "from keras.layers import Flatten,Input,Conv2D,MaxPooling2D,BatchNormalization,Activation,GlobalAveragePooling2D,ZeroPadding2D,Dense,Add,Dropout\n",
    "from keras.models import Model\n",
    "from keras.engine import get_source_inputs\n",
    "from keras.utils import to_categorical, get_file\n",
    "from keras.layers.merge import concatenate,add\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from distutils.version import StrictVersion\n",
    "\n",
    "if StrictVersion(keras.__version__) < StrictVersion('2.2.0'):\n",
    "    from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "else:\n",
    "    from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "\n",
    "def build_resnet(\n",
    "     repetitions=(2, 2, 2, 2),\n",
    "     include_top=True,\n",
    "     input_tensor=None,\n",
    "     input_shape=None,\n",
    "     classes=1000,\n",
    "     block_type='usual'):\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=101,\n",
    "                                      data_format='channels_last',\n",
    "                                      require_flatten=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape, name='data')\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    \n",
    "    # get parameters for model layers\n",
    "    no_scale_bn_params = get_bn_params(scale=False)\n",
    "    bn_params = get_bn_params()\n",
    "    conv_params = get_conv_params()\n",
    "    init_filters = 64\n",
    "\n",
    "    if block_type == 'basic':\n",
    "        conv_block = basic_conv_block\n",
    "        identity_block = basic_identity_block\n",
    "    else:\n",
    "        conv_block = usual_conv_block\n",
    "        identity_block = usual_identity_block\n",
    "    \n",
    "    # resnet bottom\n",
    "    x = BatchNormalization(name='bn_data', **no_scale_bn_params)(img_input)\n",
    "    x = ZeroPadding2D(padding=(3, 3))(x)\n",
    "    x = Conv2D(init_filters, (7, 7), strides=(2, 2), name='conv0', **conv_params)(x)\n",
    "    x = BatchNormalization(name='bn0', **bn_params)(x)\n",
    "    x = Activation('relu', name='relu0')(x)\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', name='pooling0')(x)\n",
    "    \n",
    "    # resnet body\n",
    "    for stage, rep in enumerate(repetitions):\n",
    "        for block in range(rep):\n",
    "            \n",
    "            filters = init_filters * (2**stage)\n",
    "            \n",
    "            # first block of first stage without strides because we have maxpooling before\n",
    "            if block == 0 and stage == 0:\n",
    "                x = conv_block(filters, stage, block, strides=(1, 1))(x)\n",
    "                \n",
    "            elif block == 0:\n",
    "                x = conv_block(filters, stage, block, strides=(2, 2))(x)\n",
    "                \n",
    "            else:\n",
    "                x = identity_block(filters, stage, block)(x)\n",
    "                \n",
    "    x = BatchNormalization(name='bn1', **bn_params)(x)\n",
    "    x = Activation('relu', name='relu1')(x)\n",
    "\n",
    "    # resnet top\n",
    "    if include_top:\n",
    "        x = GlobalAveragePooling2D(name='pool1')(x)\n",
    "        x = Dense(classes, name='fc1')(x)\n",
    "        x = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "        \n",
    "    # Create model.\n",
    "    model = Model(inputs, x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_weights(weights_collection, model_name, dataset, include_top):\n",
    "    w = list(filter(lambda x: x['model'] == model_name, weights_collection))\n",
    "    w = list(filter(lambda x: x['dataset'] == dataset, w))\n",
    "    w = list(filter(lambda x: x['include_top'] == include_top, w))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(weights_collection, model, dataset, classes, include_top):\n",
    "    weights = find_weights(weights_collection, model.name, dataset, include_top)\n",
    "\n",
    "    if weights:\n",
    "        weights = weights[0]\n",
    "\n",
    "        if include_top and weights['classes'] != classes:\n",
    "            raise ValueError('If using `weights` and `include_top`'\n",
    "                             ' as true, `classes` should be {}'.format(weights['classes']))\n",
    "\n",
    "        weights_path = get_file(weights['name'],\n",
    "                                weights['url'],\n",
    "                                cache_subdir='models',\n",
    "                                md5_hash=weights['md5'])\n",
    "\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('There is no weights for such configuration: ' +\n",
    "                         'model = {}, dataset = {}, '.format(model.name, dataset) +\n",
    "                         'classes = {}, include_top = {}.'.format(classes, include_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_block_names(stage, block):\n",
    "    name_base = 'stage{}_unit{}_'.format(stage + 1, block + 1)\n",
    "    conv_name = name_base + 'conv'\n",
    "    bn_name = name_base + 'bn'\n",
    "    relu_name = name_base + 'relu'\n",
    "    sc_name = name_base + 'sc'\n",
    "    return conv_name, bn_name, relu_name, sc_name\n",
    "\n",
    "\n",
    "def basic_identity_block(filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        x = Add()([x, input_tensor])\n",
    "        return x\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for Resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_conv_block(filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        shortcut = x\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        shortcut = Conv2D(filters, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n",
    "        x = Add()([x, shortcut])\n",
    "        return x\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def usual_conv_block(filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        shortcut = x\n",
    "        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '3')(x)\n",
    "        x = Conv2D(filters*4, (1, 1), name=conv_name + '3', **conv_params)(x)\n",
    "\n",
    "        shortcut = Conv2D(filters*4, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n",
    "        x = Add()([x, shortcut])\n",
    "        return x\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def usual_identity_block(filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '3')(x)\n",
    "        x = Conv2D(filters*4, (1, 1), name=conv_name + '3', **conv_params)(x)\n",
    "\n",
    "        x = Add()([x, input_tensor])\n",
    "        return x\n",
    "\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_params(**params):\n",
    "    default_conv_params = {\n",
    "        'kernel_initializer': 'glorot_uniform',\n",
    "        'use_bias': False,\n",
    "        'padding': 'valid',\n",
    "    }\n",
    "    default_conv_params.update(params)\n",
    "    return default_conv_params\n",
    "\n",
    "def get_bn_params(**params):\n",
    "    default_bn_params = {\n",
    "        'axis': 3,\n",
    "        'momentum': 0.99,\n",
    "        'epsilon': 2e-5,\n",
    "        'center': True,\n",
    "        'scale': True,\n",
    "    }\n",
    "    default_bn_params.update(params)\n",
    "    return default_bn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_collection = [\n",
    "    # ResNet34\n",
    "    {\n",
    "        'model': 'resnet34',\n",
    "        'dataset': 'imagenet',\n",
    "        'classes': 1000,\n",
    "        'include_top': True,\n",
    "        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000.h5',\n",
    "        'name': 'resnet34_imagenet_1000.h5',\n",
    "        'md5': '2ac8277412f65e5d047f255bcbd10383',\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'model': 'resnet34',\n",
    "        'dataset': 'imagenet',\n",
    "        'classes': 1000,\n",
    "        'include_top': False,\n",
    "        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5',\n",
    "        'name': 'resnet34_imagenet_1000_no_top.h5',\n",
    "        'md5': '8caaa0ad39d927cb8ba5385bf945d582',\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet34(input_shape, input_tensor=None, weights=None, classes=1000, include_top=True):\n",
    "    model = build_resnet(input_tensor=input_tensor,\n",
    "                         input_shape=input_shape,\n",
    "                         repetitions=(3, 4, 6, 3),\n",
    "                         classes=classes,\n",
    "                         include_top=include_top,\n",
    "                         block_type='basic')\n",
    "    model.name = 'resnet34'\n",
    "\n",
    "    if weights:\n",
    "        load_model_weights(weights_collection, model, weights, classes, include_top)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image processing and Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures = {'wave':'Hello',\n",
    "            'pointup':'Volume_Up',\n",
    "            'rockon':'Play',\n",
    "            'pointright':'Next',\n",
    "            'skip':'Next'\n",
    "            }\n",
    "\n",
    "gestures_map = {'Hello' : 0,\n",
    "                'Volume_Up': 1,\n",
    "                'Play': 2,\n",
    "                'Next': 3,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(path):\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((224, 224))\n",
    "    img = np.array(img)\n",
    "    return img\n",
    "\n",
    "def process_data(X_data, y_data):\n",
    "    X_data = np.array(X_data, dtype='float32')\n",
    "    if rgb:\n",
    "        pass\n",
    "    else:\n",
    "        X_data = np.stack((X_data,)*3, axis=-1)\n",
    "    X_data /= 255\n",
    "    y_data = np.array(y_data)\n",
    "    y_data = to_categorical(y_data, num_classes=4)\n",
    "    return X_data, y_data\n",
    "\n",
    "def walk_file_tree(relative_path):\n",
    "    X_data = []\n",
    "    y_data = [] \n",
    "    for directory, subdirectories, files in os.walk(relative_path):\n",
    "        for file in files:\n",
    "            if not file.startswith('.') and (not file.startswith('C_')):\n",
    "                path = os.path.join(directory, file)\n",
    "                gesture_name = gestures[file.split('_')[0]]\n",
    "                y_data.append(gestures_map[gesture_name])\n",
    "                X_data.append(process_image(path))   \n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    X_data, y_data = process_data(X_data, y_data)\n",
    "    return X_data, y_data\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self):\n",
    "        self.X_data = []\n",
    "        self.y_data = []\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.X_data, self.y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(img_path, train_batch_size, directory, prefix, generation_count):\n",
    "    try:\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255, \n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2) \n",
    "\n",
    "        x = load_img(img_path)\n",
    "        x = img_to_array(x)\n",
    "        x = x.reshape((1, ) + x.shape)\n",
    "\n",
    "        i = 0\n",
    "        for batch in test_datagen.flow(x, batch_size=train_batch_size,\n",
    "                              save_to_dir=directory, save_prefix=prefix, save_format='png'):\n",
    "            i += 1\n",
    "            if i > generation_count:\n",
    "                return 1\n",
    "                break\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_name = ['wave', 'pointup', 'rockon', 'pointright']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relative_path = './data/'\n",
    "rgb = True\n",
    "train_batch_size = 64\n",
    "for i in range(4):\n",
    "    img_len = (sum(np.argmax(y_data, axis=1) == i))\n",
    "    for j in range(0, img_len, 10):\n",
    "        img = gesture_name[i]\n",
    "        file_path = './data' + gesture_name[i] + '_' + str(j) + '.png'\n",
    "        res = augment_data(file_path, train_batch_size, '../data/aug', gesture_name[i]+'_' + str(j), 2)\n",
    "X_data, y_data = walk_file_tree(relative_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5240, 4)\n",
      "1776.0\n",
      "1534.0\n",
      "737.0\n",
      "1193.0\n"
     ]
    }
   ],
   "source": [
    "# go = np.array(y_data)\n",
    "# count = np.bincount(go)\n",
    "print(y_data.shape)\n",
    "print(np.sum(y_data[:,0]))\n",
    "print(np.sum(y_data[:,1]))\n",
    "print(np.sum(y_data[:,2]))\n",
    "print(np.sum(y_data[:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random_integers(0, high=X_data.shape[0], size=None)Â¶\n",
    "for i in sample:\n",
    "    plt.plot(X_data[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data shape:(5240, 224, 224, 3)\n",
      "y_data shape:(5240, 4)\n",
      "[[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('X_data shape:' + str(X_data.shape))\n",
    "print('y_data shape:' + str(y_data.shape))\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_rgb, X_test_rgb, y_train_rgb, y_test_rgb = train_test_split(image_rgb, y_data, test_size = 0.2, random_state=12, stratify=y_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2, random_state=12, stratify=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './models/test_model.h5'\n",
    "imageSize = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/uditsharma025_gmail_com/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 4192 samples, validate on 1048 samples\n",
      "Epoch 1/3\n",
      "4192/4192 [==============================] - 579s 138ms/step - loss: 0.8281 - accuracy: 0.7579 - val_loss: 2.9251 - val_accuracy: 0.2281\n",
      "Epoch 2/3\n",
      "4192/4192 [==============================] - 570s 136ms/step - loss: 0.0895 - accuracy: 0.9688 - val_loss: 3.1398 - val_accuracy: 0.2281\n",
      "Epoch 3/3\n",
      "4192/4192 [==============================] - 568s 136ms/step - loss: 0.0623 - accuracy: 0.9816 - val_loss: 5.8852 - val_accuracy: 0.2281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f9abe605240>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode1 = ResNet34(input_shape=(imageSize, imageSize, 3), weights='imagenet', classes=1000,include_top=False)\n",
    "optimizer1 = optimizers.Adam()\n",
    "\n",
    "base_model = mode1  # Topless\n",
    "# Add top layer\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu', name='fc1')(x)\n",
    "x = Dense(128, activation='relu', name='fc2')(x)\n",
    "x = Dense(128, activation='relu', name='fc3')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu', name='fc4')(x)\n",
    "\n",
    "predictions = Dense(4, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Train top layer\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.Adam(),\n",
    "#               metrics=[keras.metrics.Accuracy, keras.metrics.FalsePositives, keras.metrics.FalseNegatives, keras.metrics.Recall])\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/new_temp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/uditsharma025_gmail_com/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/uditsharma025_gmail_com/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 4192 samples, validate on 1048 samples\n",
      "Epoch 1/10\n",
      "4192/4192 [==============================] - 538s 128ms/step - loss: 0.0457 - accuracy: 0.9874 - val_loss: 6.1479 - val_accuracy: 0.2281\n",
      "Epoch 2/10\n",
      "4192/4192 [==============================] - 512s 122ms/step - loss: 0.0480 - accuracy: 0.9854 - val_loss: 6.4994 - val_accuracy: 0.2281\n",
      "Epoch 3/10\n",
      "4192/4192 [==============================] - 505s 120ms/step - loss: 0.0494 - accuracy: 0.9881 - val_loss: 6.7842 - val_accuracy: 0.2281\n",
      "Epoch 4/10\n",
      "4192/4192 [==============================] - 504s 120ms/step - loss: 0.0167 - accuracy: 0.9948 - val_loss: 8.4781 - val_accuracy: 0.2281\n",
      "Epoch 5/10\n",
      "4192/4192 [==============================] - 505s 121ms/step - loss: 0.0244 - accuracy: 0.9931 - val_loss: 5.4172 - val_accuracy: 0.2281\n",
      "Epoch 6/10\n",
      "4192/4192 [==============================] - 508s 121ms/step - loss: 0.0474 - accuracy: 0.9878 - val_loss: 8.3667 - val_accuracy: 0.2281\n",
      "Epoch 7/10\n",
      "3008/4192 [====================>.........] - ETA: 1:52 - loss: 0.0147 - accuracy: 0.9957"
     ]
    }
   ],
   "source": [
    "model = load_model('models/new_temp.h5')\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/new_temp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/new_temp.h5')\n",
    "pred = model.predict(X_test)\n",
    "predictions = np.argmax(pred, axis=1)\n",
    "y_test_bool = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       355\n",
      "           1       0.00      0.00      0.00       307\n",
      "           2       0.00      0.00      0.00       147\n",
      "           3       0.23      1.00      0.37       239\n",
      "\n",
      "    accuracy                           0.23      1048\n",
      "   macro avg       0.06      0.25      0.09      1048\n",
      "weighted avg       0.05      0.23      0.08      1048\n",
      "\n",
      "[[  0   0   0 355]\n",
      " [  0   0   0 307]\n",
      " [  0   0   0 147]\n",
      " [  0   0   0 239]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uditsharma025_gmail_com/.local/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_bool, predictions))\n",
    "print(confusion_matrix(y_test_bool, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4192 samples, validate on 1048 samples\n",
      "Epoch 1/10\n",
      "4192/4192 [==============================] - 555s 132ms/step - loss: 0.0552 - accuracy: 0.9840 - val_loss: 6.5287 - val_accuracy: 0.2281\n",
      "Epoch 2/10\n",
      "4192/4192 [==============================] - 567s 135ms/step - loss: 0.0267 - accuracy: 0.9919 - val_loss: 4.5959 - val_accuracy: 0.2281\n",
      "Epoch 3/10\n",
      "4192/4192 [==============================] - 559s 133ms/step - loss: 0.0412 - accuracy: 0.9907 - val_loss: 3.5876 - val_accuracy: 0.2281\n",
      "Epoch 4/10\n",
      "4192/4192 [==============================] - 554s 132ms/step - loss: 0.0263 - accuracy: 0.9919 - val_loss: 4.3987 - val_accuracy: 0.2281\n",
      "Epoch 5/10\n",
      "4192/4192 [==============================] - 551s 132ms/step - loss: 0.0148 - accuracy: 0.9959 - val_loss: 4.6478 - val_accuracy: 0.2281\n",
      "Epoch 6/10\n",
      "4192/4192 [==============================] - 551s 132ms/step - loss: 0.0099 - accuracy: 0.9959 - val_loss: 5.3321 - val_accuracy: 0.2281\n",
      "Epoch 7/10\n",
      "4192/4192 [==============================] - 548s 131ms/step - loss: 0.0422 - accuracy: 0.9902 - val_loss: 5.7347 - val_accuracy: 0.2281\n",
      "Epoch 8/10\n",
      "4192/4192 [==============================] - 561s 134ms/step - loss: 0.0461 - accuracy: 0.9912 - val_loss: 3.0561 - val_accuracy: 0.2281\n",
      "Epoch 9/10\n",
      "4192/4192 [==============================] - 570s 136ms/step - loss: 0.0209 - accuracy: 0.9952 - val_loss: 4.5709 - val_accuracy: 0.2281\n",
      "Epoch 10/10\n",
      "4192/4192 [==============================] - 565s 135ms/step - loss: 0.0172 - accuracy: 0.9962 - val_loss: 4.4768 - val_accuracy: 0.2281\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-83f8488f2254>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/new_temp.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=1,)\n",
    "model.save('models/new_temp.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('Loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/new_temp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0 1048]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       355\n",
      "           1       0.00      0.00      0.00       307\n",
      "           2       0.00      0.00      0.00       147\n",
      "           3       0.23      1.00      0.37       239\n",
      "\n",
      "    accuracy                           0.23      1048\n",
      "   macro avg       0.06      0.25      0.09      1048\n",
      "weighted avg       0.05      0.23      0.08      1048\n",
      "\n",
      "[[  0   0   0 355]\n",
      " [  0   0   0 307]\n",
      " [  0   0   0 147]\n",
      " [  0   0   0 239]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "print(np.bincount(y_pred_bool))\n",
    "y_test_bool = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_test_bool, y_pred_bool))\n",
    "print(confusion_matrix(y_test_bool, y_pred_bool))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
